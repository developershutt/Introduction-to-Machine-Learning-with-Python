{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Advanced Tokenization, Stemming, and Lemmatization\n",
    "\n",
    "As mentioned previously, the feature extraction in the CountVectorizer and Tfidf Vectorizer is relatively simple, and much more elaborate methods are possible. One particular step that is often improved in more sophisticated text-processing applications is the first step in the bag-of-words model: tokenization. This step defines what constitutes a word for the purpose of feature extraction.\n",
    "\n",
    "We saw earlier that the vocabulary often contains singular and plural versions of some words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and \"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only increase overfitting, and not allow the model to fully exploit the training data. Similarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace ment\", \"replaces\", and \"replacing\", which are different verb forms and a noun relating to the verb “to replace.” Similarly to having singular and plural forms of a noun, treating different verb forms and related words as distinct tokens is disadvantageous for building a model that generalizes well.\n",
    "\n",
    "This problem can be overcome by representing each word using its word stem, which involves identifying (or conflating) all the words that have the same word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as stemming. If instead a dictionary of known word forms is used (an explicit and human-verified system), and the role of the word in the sentence is taken into account, the process is referred to as lemmatization and the standardized form of the word is referred to as the lemma. Both processing methods, lemmatization and stemming, are forms of normalization that try to extract some normal form of a word.\n",
    "\n",
    "\n",
    "To get a better understanding of normalization, let’s compare a method for stemming —the Porter stemmer, a widely used collection of heuristics (here imported from the nltk package)—to lemmatization as implemented in the spacy package:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and load dataset\n",
    "\n",
    "# sklearn load_files function provide to load dataset from external file\n",
    "from sklearn.datasets import load_files\n",
    "review_train=load_files('dataset/')\n",
    "# load_file returns a bunch, containing training texts and training labels\n",
    "text_train,y_train=review_train.data,review_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scapy english-language model\n",
    "en_nlp = spacy.load('en')\n",
    "# instantiate nltk's Porter stemmer\n",
    "stemmer=nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_normalization(doc):\n",
    "    # tokenize document in spacy\n",
    "    doc_spacy=en_nlp(doc)\n",
    "    # print lemmas found by spacy\n",
    "    print('Lemmatization:')\n",
    "    print([token.lemma_ for token in doc_spacy])\n",
    "    \n",
    "    # Print token found by porter stemmer\n",
    "    print(\"Stemming:\")\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "['-PRON-', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', '-PRON-', 'be', 'scared', 'of', 'meet', 'the', 'client', 'tomorrow']\n",
      "Stemming:\n",
      "['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', 'am', 'scare', 'of', 'meet', 'the', 'client', 'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "compare_normalization(u'Our meeting today was worse than yesterday, '\n",
    "                     \"I'm scared of meeting the clients tomorrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is always restricted to trimming the word to a stem, so \"was\" becomes\n",
    "\"wa\" , while lemmatization can retrieve the correct base verb form, \"be\" . Similarly,\n",
    "lemmatization can normalize \"worse\" to \"bad\" , while stemming produces \"wors\" .\n",
    "Another major difference is that stemming reduces both occurrences of \"meeting\" to\n",
    "\"meet\" . Using lemmatization, the first occurrence of \"meeting\" is recognized as a noun and left as is, while the second occurrence is recognized as a verb and reduced\n",
    "to \"meet\" . In general, lemmatization is a much more involved process than stem‐\n",
    "ming, but it usually produces better results than stemming when used for normaliz‐\n",
    "ing tokens for machine learning.\n",
    "\n",
    "While scikit-learn implements neither form of normalization, CountVectorizer\n",
    "allows specifying your own tokenizer to convert each document into a list of tokens\n",
    "using the tokenizer parameter. We can use the lemmatization from spacy to create a\n",
    "callable that will take a string and produce a list of lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technicality: we want to use the regexp-based tokenizer \n",
    "# that is used by CountVectorizer and only use the lemmatization \n",
    "# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer) \n",
    "# with the regexp-based tokenization. \n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#regxp used in CountVectorizer\n",
    "regxp=re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "\n",
    "# load spacy language model and save old tokenzier\n",
    "en_nlp=spacy.load('en')\n",
    "old_tokenizer=en_nlp.tokenizer\n",
    "# replace the tokenizer using the spacy document processing pipeline\n",
    "\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "# define a count vectorizer with the custom tokenizer\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the data and inspact the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lemma.shape: (25000, 23764)\n"
     ]
    }
   ],
   "source": [
    "# transform text_train using CountVectorizer\n",
    "X_train_lemma=lemma_vect.fit_transform(text_train)\n",
    "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (25000, 27272)\n"
     ]
    }
   ],
   "source": [
    "# standard CountVectorizer for refernce\n",
    "vect=CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train=vect.transform(text_train)\n",
    "print('X_train.shape: {}'.format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output, lemmatization reduced the number of features from\n",
    "27,271 (with the standard CountVectorizer processing) to 23,764. Lemmatization\n",
    "can be seen as a kind of regularization, as it conflates certain features. Therefore, we\n",
    "expect lemmatization to improve performance most when the dataset is small. To\n",
    "illustrate how lemmatization can help, we will use StratifiedShuffleSplit for\n",
    "cross-validation, using only 1% of the data as training data and the rest as test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score (standard CountVectorizer): 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score (lemmatization): 0.719\n"
     ]
    }
   ],
   "source": [
    "# build a grid search using only 1% of the data as the training set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.99,train_size=0.01, random_state=0)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
    "\n",
    "# perform grid search with standard CountVectorizer\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"Best cross-validation score (standard CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
    "\n",
    "# perform grid search with lemmatization\n",
    "grid.fit(X_train_lemma,y_train)\n",
    "print(\"Best cross-validation score (lemmatization): {:.3f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, lemmatization provided a modest improvement in performance. As with\n",
    "many of the different feature extraction techniques, the result varies depending on\n",
    "the dataset. Lemmatization and stemming can sometimes help in building better (or\n",
    "at least more compact) models, so we suggest you give these techniques a try when\n",
    "trying to squeeze out the last bit of performance on a particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
